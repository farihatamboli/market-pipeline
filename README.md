# ðŸ“ˆ Market Data Pipeline

A real-time market data pipeline with anomaly detection, a live web dashboard, and a signal backtester â€” built for extensibility and production-readiness.

---

## ðŸš€ Try It Live

| Method | Effort | What you get |
|--------|--------|--------------|
| [![GitHub Pages](https://img.shields.io/badge/GitHub%20Pages-Demo-222?logo=github&logoColor=white)](https://farihatamboli.github.io/market-pipeline/) | Zero setup | Static demo with simulated live data |
| [![Launch Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/farihatamboli/market-pipeline/HEAD?urlpath=proxy/5050/) | ~2 min cold start | Real Flask app, real yfinance data, in-browser |
| [![Live App](market-pipeline-production.up.railway.app) | Always on | Full deployment, real data, shareable URL |

> **Quickest:** Click GitHub Pages â€” no wait, no account needed.

---

## Features

- **Live data ingestion** via `yfinance` (free) or **Alpaca WebSocket** (real-time)
- **SQLite persistence** â€” queryable tick store, easy to swap for TimescaleDB/Postgres
- **4 signal detectors**: price spike, volume surge, volatility burst, VWAP deviation
- **Web dashboard**: live price + VWAP chart, volume bars, signal feed, multi-symbol switcher
- **Signal backtester**: Jupyter notebook with forward return analysis, hit rates, and Sharpe ratios
- **Docker**: one command to run everything
- **Full test suite** with `pytest`

---

## Quickstart

### Local (no Docker)

```bash
git clone https://github.com/farihatamboli/market-pipeline.git
cd market-pipeline
pip install -r requirements.txt

# Terminal 1 â€” pipeline (yfinance polling)
python main.py --symbols AAPL MSFT SPY NVDA TSLA --interval 60

# Terminal 2 â€” dashboard
python -m dashboard.app
# â†’ http://localhost:5050
```

### Docker (one command)

```bash
docker compose up
# â†’ Dashboard at http://localhost:5050
# â†’ Pipeline runs automatically alongside it
```

### Alpaca real-time stream

```bash
export ALPACA_API_KEY=your_key
export ALPACA_SECRET_KEY=your_secret

python main.py --symbols AAPL MSFT SPY --stream alpaca
```

Sign up free at [alpaca.markets](https://alpaca.markets) â€” no card required for paper trading.

### Backtester

```bash
cd notebooks
jupyter notebook backtest.ipynb
```

---

## Architecture

```
main.py                    CLI â€” yfinance polling or Alpaca stream
docker-compose.yml         One-command deployment
Dockerfile
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ fetcher.py         yfinance ingestion â†’ Tick dataclass
â”‚   â”œâ”€â”€ alpaca_stream.py   Real-time WebSocket stream (Alpaca)
â”‚   â”œâ”€â”€ storage.py         SQLite persistence layer
â”‚   â”œâ”€â”€ signals.py         Stateless anomaly detectors
â”‚   â”œâ”€â”€ alerts.py          Console + file + Slack alert channels
â”‚   â””â”€â”€ pipeline.py        Polling orchestration loop
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ app.py             Flask + SSE streaming server
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ dashboard.html Live web UI (Chart.js)
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ backtest.ipynb     Signal quality analysis
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_signals.py    pytest unit tests
â””â”€â”€ binder/                Binder config (live demo)
```

---

## Signals

| Signal | Trigger | Configurable |
|--------|---------|-------------|
| `PRICE_SPIKE` | Price > 2.5Ïƒ from rolling mean | `price_spike_zscore` |
| `VOLUME_SURGE` | Volume > 3Ã— rolling average | `volume_surge_multiplier` |
| `VOLATILITY_BURST` | H-L range > 2.5Ã— rolling average | `volatility_burst_multiplier` |
| `VWAP_DEVIATION` | Price > 0.5% from session VWAP | `vwap_deviation_pct` |

```python
detector = SignalDetector(
    price_spike_zscore          = 3.0,   # tighten/loosen sensitivity
    volume_surge_multiplier     = 4.0,
    volatility_burst_multiplier = 2.0,
    vwap_deviation_pct          = 1.0,
)
```

---

## Live Demo Setup

### Option 1 â€” GitHub Pages (zero effort)

1. Repo â†’ **Settings â†’ Pages â†’ Source: main branch / root**
2. Copy `dashboard/templates/dashboard.html` â†’ repo root as `index.html`
3. Update badge URL: `https://farihatamboli.github.io/market-pipeline/`

### Option 2 â€” Binder (~2 min cold start)

The `binder/` folder is already configured. Just update the badge URL with your username:
```
https://mybinder.org/v2/gh/farihatamboli/market-pipeline/HEAD?urlpath=proxy/5050/
```

### Option 3 â€” Railway (always-on, no card required)

1. [railway.app](https://railway.app) â†’ **New Project â†’ Deploy from GitHub**
2. Select this repo
3. **Variables** tab â†’ add `PORT = 5050`
4. **Settings â†’ Deploy â†’ Start Command:**
   ```
   python -m dashboard.app
   ```
5. **Settings â†’ Networking â†’ Generate Domain** â†’ copy URL into badge

---

## Running Tests

```bash
pytest tests/ -v
```

---

## Extending

**New signal** â€” add a `_check_*` method in `src/signals.py`, call it from `detect()`.

**Slack alerts** â€” set `SLACK_WEBHOOK_URL` env var and uncomment the Slack channel in `src/alerts.py`.

**TimescaleDB** â€” implement the `insert_tick` / `get_recent` / `get_range` interface in a new `TimescaleStore` class and swap it in.

**More symbols** â€” just pass them via `--symbols`. No other changes needed.

---

## Tech Stack

- Python 3.11+
- `yfinance` + `websocket-client` (Alpaca) for market data
- `flask` + Server-Sent Events for live dashboard
- `sqlite3` (stdlib) for storage
- `Chart.js` for charting
- `pandas` + `matplotlib` for backtesting
- `pytest` for testing
- `Docker` + `docker-compose` for deployment

---

## Design Notes

**SSE over WebSockets** â€” the dashboard uses Server-Sent Events deliberately. SSE is plain HTTP, auto-reconnects on failure, works through proxies and load balancers, and is the right fit for one-directional server â†’ browser streaming. No extra library needed.

**Stateless `SignalDetector`** â€” pure function signature `(tick, history) â†’ signals`. No hidden state, trivially parallelisable across symbols, and unit-testable without mocking.

**Swappable data sources** â€” `MarketFetcher` (yfinance) and `AlpacaStream` both produce the same `Tick` dataclass, so the storage and signal layers are completely agnostic to the data source.
